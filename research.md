---
layout: research
title: "Research"
---
<p align="justify">
<p style="font-size:30px"><strong>Neural mechanisms of speech categorization</strong></p><br>

Successful perception of speech requires that the human brain to first encode the acoustic signal, and subsequently transform this continuous information into discrete, well-formed categories, a process known as categorical perception. Skilled categorization is particularly important in the context of spoken and written language as evident by its integral role in reading acquisition and auditory-based learning disorders (e.g., dyslexia, specific language impairment). Processing of speech is heavily dependent on the neural structures of the auditory pathway from sensory receptors of hearing in the peripheral nervous system to subcortical (brainstem) and cortical nuclei of the central nervous system. I have employed a system-level approach to comprehensively measure neurophysiological activities from multiple levels of the auditory system (subcortical and cortical) to address two central questions <u>(1) when and how the auditory system transforms continuous acoustic signals into discrete categories? (2) whether and how individual differences of the neural mechanisms predict behavioral categorization?</u> </p>

More information:<br>
<i>Ou, Jinghua, & Yu, A. C. (2021). Neural correlates of individual differences in speech categorisation: evidence from subcortical, cortical, and behavioural measures. Language, Cognition and Neuroscience, 37(3), 269-284.</i><br><br>
 
<p align="justify">
<strong>Perceptual strategies in phonetic cue weighting</strong><br>

In speech, multiple cues within the acoustic signal often define a given category. However, not all cues are equal. Listeners are found to weigh each of these dimensions differentially during categorization. For instance, even if listeners consistently weighted VOT more than F0 for the stop initial voicing contrasts in English, significant individual variations were still observed in the use of cues. I employed the Visual World Paradigm (VWP) in which listeners match speech sounds to picture while eye-movements are monitored to ascertain <u>(1) whether individual differences in categorization are driven by cue integration; (2) whether the likelihood of integrating multiple cues during perception is driven by the differences in how listeners sample evidence from the acoustic environment (i.e. a cascade vs. buffer processing strategy).</u></p> 

More information:<br>
<i>Ou, J., Yu, A. C. L, & Xiang, M. (2021). Individual differences in categorization gradience as predicted by online processing of phonetic cues during spoken word recognition: Evidence from eye movements. Cognitive Science, 45(3), e12948.</i><br><br>

<p align="justify">
<strong>Neural oscillations in speech perception</strong><br>
 
Auditory neuroscience has provided strong evidence that neural oscillations synchronize to the rhythm of speech stimuli, and oscillations at different frequencies have been linked to processing of different language structures. Specific to speech processing, it has been theorized that slow neural oscillations (i.e. theta band, 4–8 Hz) temporally align to and encode the syllable structures, which are characterized by statistical regularities at a scale of 100–200 ms (i.e. similar to a theta cycle) in speech. On the other hand, neural oscillations at higher frequencies (i.e. low-gamma, 25–35Hz) are thought to encode the phonetic structures of speech. I applied time-frequency analysis to electrophysiological responses to a lexical tone contrast from two groups of healthy tone-language speakers who differ in their tone perception and production ability, in order to ascertain <i>(1) whether individual differences in tone perception are related to neural oscillations; (2) is there a link between perception and production to the extent that neural oscillationatory differences may predict production variations among individuals.</i></p>

More information:<br>
<i>Ou, J., & Law, S. P. (2018). Induced gamma oscillations index individual differences in speech sound perception and production. Neuropsychologia, 121, 28-36.</i>  
<i>Ou, J., & Law, S. P. (2019). Top-down and bottom-up mechanisms as reflected by beta and gamma oscillations in speech perception: An individual-difference approach. Brain and Language, 199, 104700.</i><br><br>

<p align="justify">
<strong>Individual differences in perception and production in the context of sound change</strong><br>
 
Sound change takes place in the mind of individual language user. By studying variations of individual language system in terms of perception and production, we can begin to elucidate the neurocognitive factors that condition actuation of change in some speakers but not others. The subtle speech-processing differences bestowed by heterogeneity in neurocognitive capacity may result in bias for certain linguistic variants, which may become seeds for language change. Sociocultural transmission may then act to amplify these subtle cognitive biases leading to widespread change in the community. I pursued this question by examining the range of individual tonal structures possible among Hong Kong Cantonese speakers – <u>whether and how tonal distinctions are maintained in perception and production, moreover linking the cognitive capacity of individuals and how they process language to their idiosyncratic structures.</u></p>

More information:<br> 
<i>Ou, J., Law, S. P., & Fung, R. (2015). Relationship between individual differences in speech processing and cognitive function. Psychonomic Bulletin and Review, 22(5): 1451-1457.</i>
 
<i>Ou, J., & Law, S. P. (2016). Individual differences in processing pitch contour and rise time in adults: A behavioral and electrophysiological study of Cantonese merging. Journal of the Acoustic Society of America, 139(6), 3226-3237.</i>

<i>Ou, J., & Law, S. P. (2017). Cognitive basis of individual differences in speech perception, production and representations: The role of domain general attentional-switching. Attention, Perception, & Psychophysics, 79, 945-963.</i>
 

